{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from datautils import *\n",
    "from gptq import *\n",
    "from modelutils import *\n",
    "from quant import *\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from datautils import *\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "parser.add_argument(\n",
    "    'model', type=str,\n",
    "    help='OPT model to load; pass `facebook/opt-X`.'\n",
    ")\n",
    "parser.add_argument(\n",
    "    'dataset', type=str, choices=['wikitext2', 'ptb', 'c4'],\n",
    "    help='Where to extract calibration data from.'\n",
    ")\n",
    "parser.add_argument(\n",
    "    '--seed',\n",
    "    type=int, default=0, help='Seed for sampling the calibration data.'\n",
    ")\n",
    "parser.add_argument(\n",
    "    '--nsamples', type=int, default=128,\n",
    "    help='Number of calibration data samples.'\n",
    ")\n",
    "parser.add_argument(\n",
    "    '--percdamp', type=float, default=.01,\n",
    "    help='Percent of the average Hessian diagonal to use for dampening.'\n",
    ")\n",
    "parser.add_argument(\n",
    "        '--nearest', action='store_true',\n",
    "        help='Whether to run the RTN baseline.'\n",
    "    ) \n",
    "parser.add_argument(\n",
    "    '--wbits', type=int, default=16, choices=[2, 3, 4, 16],\n",
    "    help='#bits to use for quantization; use 16 for evaluating base model.'\n",
    ")\n",
    "parser.add_argument(\n",
    "    '--trits', action='store_true',\n",
    "    help='Whether to use trits for quantization.'\n",
    ")\n",
    "parser.add_argument(\n",
    "    '--groupsize', type=int, default=-1,\n",
    "    help='Groupsize to use for quantization; default uses full row.'\n",
    ")\n",
    "parser.add_argument(\n",
    "    '--sym', action='store_true',\n",
    "    help='Whether to perform symmetric quantization.'\n",
    ")\n",
    "parser.add_argument(\n",
    "    '--save', type=str, default='',\n",
    "    help='Save quantized checkpoint under this name.'\n",
    ")\n",
    "parser.add_argument(\n",
    "    '--load', type=str, default='',\n",
    "    help='Load quantized model.'\n",
    ")\n",
    "parser.add_argument(\n",
    "    '--benchmark', type=int, default=0,\n",
    "    help='Number of tokens to use for benchmarking.'\n",
    ")\n",
    "parser.add_argument(\n",
    "    '--check', action='store_true',\n",
    "    help='Whether to compute perplexity during benchmarking for verification.'\n",
    ")\n",
    "parser.add_argument(\n",
    "    '--new-eval', action='store_true',\n",
    "    help='Whether to use the new PTB and C4 eval.'\n",
    ")\n",
    "parser.add_argument(\n",
    "    '--faster-kernel', action='store_true',\n",
    "    help='Whether to use the new faster kernel for benchmarking.'\n",
    ")\n",
    "parser.add_argument(\n",
    "    '--act-order', action='store_true',\n",
    "    help='Whether to apply the activation order GPTQ heuristic'\n",
    ")\n",
    "parser.add_argument(\n",
    "    '--static-groups', action='store_true',\n",
    "    help='Whether to use static groups; recommended when using `--actorder` for more efficient inference.'\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "args = parser.parse_args()\n",
    "\n",
    "\n",
    "dataloader, testloader = get_loaders(\n",
    "    args.dataset, nsamples=args.nsamples, seed=args.seed, model=args.model, seqlen=model.seqlen\n",
    ")\n",
    "if args.load:\n",
    "    exit()\n",
    "datasets = ['wikitext2', 'ptb', 'c4'] \n",
    "if args.new_eval:\n",
    "    datasets = ['wikitext2', 'ptb-new', 'c4-new']\n",
    "for dataset in datasets: \n",
    "    dataloader, testloader = get_loaders(\n",
    "        dataset, seed=args.seed, model=args.model, seqlen=model.seqlen\n",
    "    )\n",
    "    print(dataset)\n",
    "if args.save:\n",
    "    opt_pack3(model, quantizers)\n",
    "    torch.save(model.state_dict(), args.save) \n",
    "print(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BloomModel\n",
    "\n",
    "model = BloomModel.from_pretrained(\"bigscience/bloom-7b1\")\n",
    "\n",
    "\n",
    "directory_on_my_computer=\"./\"\n",
    "\n",
    "model.save_pretrained(\"directory_on_my_computer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "PackageNotFoundError",
     "evalue": "No package metadata was found for bitsandbytes",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPackageNotFoundError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m/data/home/dujianhua/gptq/test.ipynb 单元格 6\u001b[0m line \u001b[0;36m4\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B10.101.4.37/data/home/dujianhua/gptq/test.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B10.101.4.37/data/home/dujianhua/gptq/test.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m \u001b[39mimport\u001b[39;00m BitsAndBytesConfig\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B10.101.4.37/data/home/dujianhua/gptq/test.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m quantization_config \u001b[39m=\u001b[39m BitsAndBytesConfig(\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B10.101.4.37/data/home/dujianhua/gptq/test.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m    load_in_4bit\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B10.101.4.37/data/home/dujianhua/gptq/test.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m    bnb_4bit_compute_dtype\u001b[39m=\u001b[39;49mtorch\u001b[39m.\u001b[39;49mbfloat16\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B10.101.4.37/data/home/dujianhua/gptq/test.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.10/site-packages/transformers/utils/quantization_config.py:212\u001b[0m, in \u001b[0;36mBitsAndBytesConfig.__init__\u001b[0;34m(self, load_in_8bit, load_in_4bit, llm_int8_threshold, llm_int8_skip_modules, llm_int8_enable_fp32_cpu_offload, llm_int8_has_fp16_weight, bnb_4bit_compute_dtype, bnb_4bit_quant_type, bnb_4bit_use_double_quant, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    210\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mbnb_4bit_compute_dtype must be a string or a torch.dtype\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 212\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpost_init()\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.10/site-packages/transformers/utils/quantization_config.py:238\u001b[0m, in \u001b[0;36mBitsAndBytesConfig.post_init\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    235\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbnb_4bit_use_double_quant, \u001b[39mbool\u001b[39m):\n\u001b[1;32m    236\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mbnb_4bit_use_double_quant must be a boolean\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 238\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mload_in_4bit \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m version\u001b[39m.\u001b[39mparse(importlib\u001b[39m.\u001b[39;49mmetadata\u001b[39m.\u001b[39;49mversion(\u001b[39m\"\u001b[39;49m\u001b[39mbitsandbytes\u001b[39;49m\u001b[39m\"\u001b[39;49m)) \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m version\u001b[39m.\u001b[39mparse(\n\u001b[1;32m    239\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m0.39.0\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    240\u001b[0m ):\n\u001b[1;32m    241\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    242\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m4 bit quantization requires bitsandbytes>=0.39.0 - please upgrade your bitsandbytes version\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    243\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.10/importlib/metadata/__init__.py:996\u001b[0m, in \u001b[0;36mversion\u001b[0;34m(distribution_name)\u001b[0m\n\u001b[1;32m    989\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mversion\u001b[39m(distribution_name):\n\u001b[1;32m    990\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Get the version string for the named package.\u001b[39;00m\n\u001b[1;32m    991\u001b[0m \n\u001b[1;32m    992\u001b[0m \u001b[39m    :param distribution_name: The name of the distribution package to query.\u001b[39;00m\n\u001b[1;32m    993\u001b[0m \u001b[39m    :return: The version string for the package as defined in the package's\u001b[39;00m\n\u001b[1;32m    994\u001b[0m \u001b[39m        \"Version\" metadata key.\u001b[39;00m\n\u001b[1;32m    995\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 996\u001b[0m     \u001b[39mreturn\u001b[39;00m distribution(distribution_name)\u001b[39m.\u001b[39mversion\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.10/importlib/metadata/__init__.py:969\u001b[0m, in \u001b[0;36mdistribution\u001b[0;34m(distribution_name)\u001b[0m\n\u001b[1;32m    963\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdistribution\u001b[39m(distribution_name):\n\u001b[1;32m    964\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Get the ``Distribution`` instance for the named package.\u001b[39;00m\n\u001b[1;32m    965\u001b[0m \n\u001b[1;32m    966\u001b[0m \u001b[39m    :param distribution_name: The name of the distribution package as a string.\u001b[39;00m\n\u001b[1;32m    967\u001b[0m \u001b[39m    :return: A ``Distribution`` instance (or subclass thereof).\u001b[39;00m\n\u001b[1;32m    968\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 969\u001b[0m     \u001b[39mreturn\u001b[39;00m Distribution\u001b[39m.\u001b[39;49mfrom_name(distribution_name)\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.10/importlib/metadata/__init__.py:548\u001b[0m, in \u001b[0;36mDistribution.from_name\u001b[0;34m(cls, name)\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[39mreturn\u001b[39;00m dist\n\u001b[1;32m    547\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 548\u001b[0m     \u001b[39mraise\u001b[39;00m PackageNotFoundError(name)\n",
      "\u001b[0;31mPackageNotFoundError\u001b[0m: No package metadata was found for bitsandbytes"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "   load_in_4bit=True,\n",
    "   bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gptq",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
