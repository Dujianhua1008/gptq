{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from datautils import *\n",
    "from gptq import *\n",
    "from modelutils import *\n",
    "from quant import *\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from datautils import *\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "parser.add_argument(\n",
    "    'model', type=str,\n",
    "    help='OPT model to load; pass `facebook/opt-X`.'\n",
    ")\n",
    "parser.add_argument(\n",
    "    'dataset', type=str, choices=['wikitext2', 'ptb', 'c4'],\n",
    "    help='Where to extract calibration data from.'\n",
    ")\n",
    "parser.add_argument(\n",
    "    '--seed',\n",
    "    type=int, default=0, help='Seed for sampling the calibration data.'\n",
    ")\n",
    "parser.add_argument(\n",
    "    '--nsamples', type=int, default=128,\n",
    "    help='Number of calibration data samples.'\n",
    ")\n",
    "parser.add_argument(\n",
    "    '--percdamp', type=float, default=.01,\n",
    "    help='Percent of the average Hessian diagonal to use for dampening.'\n",
    ")\n",
    "parser.add_argument(\n",
    "        '--nearest', action='store_true',\n",
    "        help='Whether to run the RTN baseline.'\n",
    "    ) \n",
    "parser.add_argument(\n",
    "    '--wbits', type=int, default=16, choices=[2, 3, 4, 16],\n",
    "    help='#bits to use for quantization; use 16 for evaluating base model.'\n",
    ")\n",
    "parser.add_argument(\n",
    "    '--trits', action='store_true',\n",
    "    help='Whether to use trits for quantization.'\n",
    ")\n",
    "parser.add_argument(\n",
    "    '--groupsize', type=int, default=-1,\n",
    "    help='Groupsize to use for quantization; default uses full row.'\n",
    ")\n",
    "parser.add_argument(\n",
    "    '--sym', action='store_true',\n",
    "    help='Whether to perform symmetric quantization.'\n",
    ")\n",
    "parser.add_argument(\n",
    "    '--save', type=str, default='',\n",
    "    help='Save quantized checkpoint under this name.'\n",
    ")\n",
    "parser.add_argument(\n",
    "    '--load', type=str, default='',\n",
    "    help='Load quantized model.'\n",
    ")\n",
    "parser.add_argument(\n",
    "    '--benchmark', type=int, default=0,\n",
    "    help='Number of tokens to use for benchmarking.'\n",
    ")\n",
    "parser.add_argument(\n",
    "    '--check', action='store_true',\n",
    "    help='Whether to compute perplexity during benchmarking for verification.'\n",
    ")\n",
    "parser.add_argument(\n",
    "    '--new-eval', action='store_true',\n",
    "    help='Whether to use the new PTB and C4 eval.'\n",
    ")\n",
    "parser.add_argument(\n",
    "    '--faster-kernel', action='store_true',\n",
    "    help='Whether to use the new faster kernel for benchmarking.'\n",
    ")\n",
    "parser.add_argument(\n",
    "    '--act-order', action='store_true',\n",
    "    help='Whether to apply the activation order GPTQ heuristic'\n",
    ")\n",
    "parser.add_argument(\n",
    "    '--static-groups', action='store_true',\n",
    "    help='Whether to use static groups; recommended when using `--actorder` for more efficient inference.'\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "args = parser.parse_args()\n",
    "\n",
    "\n",
    "dataloader, testloader = get_loaders(\n",
    "    args.dataset, nsamples=args.nsamples, seed=args.seed, model=args.model, seqlen=model.seqlen\n",
    ")\n",
    "if args.load:\n",
    "    exit()\n",
    "datasets = ['wikitext2', 'ptb', 'c4'] \n",
    "if args.new_eval:\n",
    "    datasets = ['wikitext2', 'ptb-new', 'c4-new']\n",
    "for dataset in datasets: \n",
    "    dataloader, testloader = get_loaders(\n",
    "        dataset, seed=args.seed, model=args.model, seqlen=model.seqlen\n",
    "    )\n",
    "    print(dataset)\n",
    "if args.save:\n",
    "    opt_pack3(model, quantizers)\n",
    "    torch.save(model.state_dict(), args.save) \n",
    "print(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BloomModel\n",
    "\n",
    "model = BloomModel.from_pretrained(\"bigscience/bloom-7b1\")\n",
    "\n",
    "\n",
    "directory_on_my_computer=\"./\"\n",
    "\n",
    "model.save_pretrained(\"directory_on_my_computer\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gptq",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
