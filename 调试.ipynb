{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6799e0f860c44640a5483837a3e37135",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/512 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset json (/home/dujh/.cache/huggingface/datasets/json/default-cd15a4614eb5e5ca/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51)\n",
      "Found cached dataset json (/home/dujh/.cache/huggingface/datasets/json/default-f4086d6aa7a60284/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51)\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import os\n",
    "import re\n",
    "from transformers import AutoTokenizer\n",
    "import random\n",
    "import torch \n",
    "def get_files_in_directory(directory, keyword):\n",
    "    file_list = []\n",
    "    for file in os.listdir(directory):\n",
    "        if re.search(keyword, file):\n",
    "            file_list.append(os.path.join(directory, file))\n",
    "    return file_list\n",
    "\n",
    "traindata_directory = './c4/realnewslike/'\n",
    "\n",
    "traindata_path = get_files_in_directory(traindata_directory, r'train')\n",
    "valdata_path = get_files_in_directory(traindata_directory, r'validation')\n",
    "\n",
    "traindata = load_dataset('json', data_files={'train': traindata_path}, split='train')\n",
    "valdata = load_dataset('json', data_files={'validation': valdata_path}, split='validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_c4(nsamples, seed, seqlen, model):\n",
    "    from datasets import load_dataset\n",
    "    import os\n",
    "    import re\n",
    "    def get_files_in_directory(directory, keyword):\n",
    "        file_list = []\n",
    "        for file in os.listdir(directory):\n",
    "            if re.search(keyword, file):\n",
    "                file_list.append(os.path.join(directory, file))\n",
    "        return file_list\n",
    "\n",
    "    traindata_directory = './c4/realnewslike/'\n",
    "\n",
    "    traindata_path = get_files_in_directory(traindata_directory, r'train')\n",
    "    valdata_path = get_files_in_directory(traindata_directory, r'validation')\n",
    "\n",
    "    traindata = load_dataset('json', data_files={'train': traindata_path}, split='train')\n",
    "    valdata = load_dataset('json', data_files={'validation': valdata_path}, split='validation')\n",
    "\n",
    "    from transformers import AutoTokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model, use_fast=False)\n",
    "\n",
    "    import random\n",
    "    random.seed(seed)\n",
    "    print(\"seed=\"+str(seed))\n",
    "    trainloader = []\n",
    "    for _ in range(nsamples):\n",
    "        while True:\n",
    "            i = random.randint(0, len(traindata) - 1)\n",
    "            trainenc = tokenizer(traindata[i]['text'], return_tensors='pt')\n",
    "            if trainenc.input_ids.shape[1] >= seqlen:\n",
    "                break\n",
    "        # print(trainenc.input_ids.shape[1] - seqlen - 1)\n",
    "        i = random.randint(0, trainenc.input_ids.shape[1] - seqlen - 1)\n",
    "        j = i + seqlen\n",
    "        inp = trainenc.input_ids[:, i:j]\n",
    "        tar = inp.clone()\n",
    "        tar[:, :-1] = -100\n",
    "        trainloader.append((inp, tar))\n",
    "\n",
    "    import random\n",
    "    random.seed(0)\n",
    "    valenc = []\n",
    "    for _ in range(256):\n",
    "        while True:\n",
    "            i = random.randint(0, len(valdata) - 1)\n",
    "            tmp = tokenizer(valdata[i]['text'], return_tensors='pt')\n",
    "            if tmp.input_ids.shape[1] >= seqlen:\n",
    "                break\n",
    "        i = random.randint(0, tmp.input_ids.shape[1] - seqlen - 1)\n",
    "        j = i + seqlen\n",
    "        valenc.append(tmp.input_ids[:, i:j])\n",
    "    valenc = torch.hstack(valenc)\n",
    "    class TokenizerWrapper:\n",
    "        def __init__(self, input_ids):\n",
    "            self.input_ids = input_ids\n",
    "    valenc = TokenizerWrapper(valenc)\n",
    "\n",
    "    return trainloader, valenc \n",
    "    # return  valenc "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ee5e0b2155744b1aea0cf5d03857c1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/512 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset json (/home/dujh/.cache/huggingface/datasets/json/default-cd15a4614eb5e5ca/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51)\n",
      "Found cached dataset json (/home/dujh/.cache/huggingface/datasets/json/default-f4086d6aa7a60284/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seed=0\n"
     ]
    }
   ],
   "source": [
    "train,vance =get_c4(128,0,1024,\"opt-125m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert type(2) in  [int,float]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wikitext2(nsamples, seed, seqlen, model):\n",
    "    import ssl\n",
    "    from datasets import load_dataset\n",
    "    # ssl._create_default_https_context = ssl._create_unverified_context\n",
    "    ssl._create_default_https_context = ssl._create_unverified_context\n",
    "    traindata = load_dataset('wikitext', 'wikitext-2-raw-v1', split='train')\n",
    "    testdata = load_dataset('wikitext', 'wikitext-2-raw-v1', split='test')\n",
    "\n",
    "    return traindata, testdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path=\"./opt-125m/\"\n",
    "train,test= get_wikitext2(128,0,2048,model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ssl\n",
    "from datasets import load_dataset\n",
    "# ssl._create_default_https_context = ssl._create_unverified_context\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "# traindata = load_dataset('wikitext', 'wikitext-2-raw-v1', split='train')\n",
    "# testdata = load_dataset('wikitext', 'wikitext-2-raw-v1', split='test')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "ConnectionError",
     "evalue": "Couldn't reach 'imdb' on the Hub (SSLError)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_dataset \u001b[39m=\u001b[39m load_dataset(\u001b[39m'\u001b[39;49m\u001b[39mimdb\u001b[39;49m\u001b[39m'\u001b[39;49m, split\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mtrain\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[0;32m~/.conda/envs/gptq/lib/python3.10/site-packages/datasets/load.py:1759\u001b[0m, in \u001b[0;36mload_dataset\u001b[0;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, ignore_verifications, keep_in_memory, save_infos, revision, use_auth_token, task, streaming, num_proc, **config_kwargs)\u001b[0m\n\u001b[1;32m   1754\u001b[0m verification_mode \u001b[39m=\u001b[39m VerificationMode(\n\u001b[1;32m   1755\u001b[0m     (verification_mode \u001b[39mor\u001b[39;00m VerificationMode\u001b[39m.\u001b[39mBASIC_CHECKS) \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m save_infos \u001b[39melse\u001b[39;00m VerificationMode\u001b[39m.\u001b[39mALL_CHECKS\n\u001b[1;32m   1756\u001b[0m )\n\u001b[1;32m   1758\u001b[0m \u001b[39m# Create a dataset builder\u001b[39;00m\n\u001b[0;32m-> 1759\u001b[0m builder_instance \u001b[39m=\u001b[39m load_dataset_builder(\n\u001b[1;32m   1760\u001b[0m     path\u001b[39m=\u001b[39;49mpath,\n\u001b[1;32m   1761\u001b[0m     name\u001b[39m=\u001b[39;49mname,\n\u001b[1;32m   1762\u001b[0m     data_dir\u001b[39m=\u001b[39;49mdata_dir,\n\u001b[1;32m   1763\u001b[0m     data_files\u001b[39m=\u001b[39;49mdata_files,\n\u001b[1;32m   1764\u001b[0m     cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[1;32m   1765\u001b[0m     features\u001b[39m=\u001b[39;49mfeatures,\n\u001b[1;32m   1766\u001b[0m     download_config\u001b[39m=\u001b[39;49mdownload_config,\n\u001b[1;32m   1767\u001b[0m     download_mode\u001b[39m=\u001b[39;49mdownload_mode,\n\u001b[1;32m   1768\u001b[0m     revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[1;32m   1769\u001b[0m     use_auth_token\u001b[39m=\u001b[39;49muse_auth_token,\n\u001b[1;32m   1770\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mconfig_kwargs,\n\u001b[1;32m   1771\u001b[0m )\n\u001b[1;32m   1773\u001b[0m \u001b[39m# Return iterable dataset in case of streaming\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m \u001b[39mif\u001b[39;00m streaming:\n",
      "File \u001b[0;32m~/.conda/envs/gptq/lib/python3.10/site-packages/datasets/load.py:1496\u001b[0m, in \u001b[0;36mload_dataset_builder\u001b[0;34m(path, name, data_dir, data_files, cache_dir, features, download_config, download_mode, revision, use_auth_token, **config_kwargs)\u001b[0m\n\u001b[1;32m   1494\u001b[0m     download_config \u001b[39m=\u001b[39m download_config\u001b[39m.\u001b[39mcopy() \u001b[39mif\u001b[39;00m download_config \u001b[39melse\u001b[39;00m DownloadConfig()\n\u001b[1;32m   1495\u001b[0m     download_config\u001b[39m.\u001b[39muse_auth_token \u001b[39m=\u001b[39m use_auth_token\n\u001b[0;32m-> 1496\u001b[0m dataset_module \u001b[39m=\u001b[39m dataset_module_factory(\n\u001b[1;32m   1497\u001b[0m     path,\n\u001b[1;32m   1498\u001b[0m     revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[1;32m   1499\u001b[0m     download_config\u001b[39m=\u001b[39;49mdownload_config,\n\u001b[1;32m   1500\u001b[0m     download_mode\u001b[39m=\u001b[39;49mdownload_mode,\n\u001b[1;32m   1501\u001b[0m     data_dir\u001b[39m=\u001b[39;49mdata_dir,\n\u001b[1;32m   1502\u001b[0m     data_files\u001b[39m=\u001b[39;49mdata_files,\n\u001b[1;32m   1503\u001b[0m )\n\u001b[1;32m   1505\u001b[0m \u001b[39m# Get dataset builder class from the processing script\u001b[39;00m\n\u001b[1;32m   1506\u001b[0m builder_cls \u001b[39m=\u001b[39m import_main_class(dataset_module\u001b[39m.\u001b[39mmodule_path)\n",
      "File \u001b[0;32m~/.conda/envs/gptq/lib/python3.10/site-packages/datasets/load.py:1218\u001b[0m, in \u001b[0;36mdataset_module_factory\u001b[0;34m(path, revision, download_config, download_mode, dynamic_modules_path, data_dir, data_files, **download_kwargs)\u001b[0m\n\u001b[1;32m   1213\u001b[0m             \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(e1, \u001b[39mFileNotFoundError\u001b[39;00m):\n\u001b[1;32m   1214\u001b[0m                 \u001b[39mraise\u001b[39;00m \u001b[39mFileNotFoundError\u001b[39;00m(\n\u001b[1;32m   1215\u001b[0m                     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCouldn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt find a dataset script at \u001b[39m\u001b[39m{\u001b[39;00mrelative_to_absolute_path(combined_path)\u001b[39m}\u001b[39;00m\u001b[39m or any data file in the same directory. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1216\u001b[0m                     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCouldn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt find \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mpath\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m on the Hugging Face Hub either: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(e1)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m: \u001b[39m\u001b[39m{\u001b[39;00me1\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1217\u001b[0m                 ) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 1218\u001b[0m             \u001b[39mraise\u001b[39;00m e1 \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1219\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1220\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mFileNotFoundError\u001b[39;00m(\n\u001b[1;32m   1221\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCouldn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt find a dataset script at \u001b[39m\u001b[39m{\u001b[39;00mrelative_to_absolute_path(combined_path)\u001b[39m}\u001b[39;00m\u001b[39m or any data file in the same directory.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1222\u001b[0m     )\n",
      "File \u001b[0;32m~/.conda/envs/gptq/lib/python3.10/site-packages/datasets/load.py:1174\u001b[0m, in \u001b[0;36mdataset_module_factory\u001b[0;34m(path, revision, download_config, download_mode, dynamic_modules_path, data_dir, data_files, **download_kwargs)\u001b[0m\n\u001b[1;32m   1165\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# noqa: catch any exception of hf_hub and consider that the dataset doesn't exist\u001b[39;00m\n\u001b[1;32m   1166\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(\n\u001b[1;32m   1167\u001b[0m         e,\n\u001b[1;32m   1168\u001b[0m         (\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1172\u001b[0m         ),\n\u001b[1;32m   1173\u001b[0m     ):\n\u001b[0;32m-> 1174\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mConnectionError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCouldn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt reach \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mpath\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m on the Hub (\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(e)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m)\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   1175\u001b[0m     \u001b[39melif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m404\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m \u001b[39mstr\u001b[39m(e):\n\u001b[1;32m   1176\u001b[0m         msg \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mDataset \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mpath\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m doesn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt exist on the Hub\u001b[39m\u001b[39m\"\u001b[39m\n",
      "\u001b[0;31mConnectionError\u001b[0m: Couldn't reach 'imdb' on the Hub (SSLError)"
     ]
    }
   ],
   "source": [
    "train_dataset = load_dataset('imdb', split='train')\n",
    "# valdata = load_dataset(        'allenai/c4', 'allenai--c4', data_files={'validation': 'en/c4-validation.00000-of-00008.json.gz'}, split='validation')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gptq",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
